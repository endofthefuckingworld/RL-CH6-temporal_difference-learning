{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "GRID_SIZE = np.zeros((4,12))\n",
    "\n",
    "ACTIONS = np.array([[-1, 0],   #0 up\n",
    "                    [1, 0],    #1 down\n",
    "                    [0, -1],   #2 left\n",
    "                    [0, 1]])   #3 right\n",
    "\n",
    "actions = np.arange(4)   #represent ACTIONS\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "START = np.array([3,0])\n",
    "\n",
    "TARGET = np.array([3,11])\n",
    "\n",
    "class Environment:\n",
    "    def step(self, State, Action):\n",
    "        pos = State + Action\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i] < 0:\n",
    "                pos[i] = max(0,pos[i])\n",
    "            else:\n",
    "                pos[i] = min(GRID_SIZE.shape[i]-1,pos[i])\n",
    "        \n",
    "        done = False\n",
    "        if (pos == TARGET).all() == True:\n",
    "            done = True\n",
    "\n",
    "        if pos[0] == 3 and 1 <= pos[1] <= 10: #fall off the cliff\n",
    "            pos = START\n",
    "            return pos, -100, done\n",
    "        else:\n",
    "            return pos, -1, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.arange(len(self.buffer))\n",
    "        sample_indices = random.choices(indices, k = batch_size)\n",
    "        samples = np.array(self.buffer, dtype = object)[sample_indices]\n",
    "    \n",
    "        return map(np.array, zip(*samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.q_network = self.build_q_network()\n",
    "        self.t_q_network = self.build_q_network()\n",
    "        self.buffer = MemoryBuffer(1000)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate = 1e-3, clipnorm=1.0)\n",
    "        self.batch_size = 16\n",
    "        self.epsilon = 1.0\n",
    "        # timestep in an episode\n",
    "        self.frame_count = 0\n",
    "        self.gamma = 1.0\n",
    "    \n",
    "    def build_q_network(self):\n",
    "        # Network architecture\n",
    "        inputs = keras.Input(shape = self.n_states)\n",
    "        x = layers.Dense(units = 8, activation = 'relu')(inputs)\n",
    "        q_value = layers.Dense(units = self.n_actions)(x)\n",
    "\n",
    "        return keras.Model(inputs = inputs, outputs = q_value)\n",
    "    \n",
    "    def ε_greedy_policy(self, t, state):\n",
    "        # exploration and exploitation\n",
    "        self.epsilon = 1.0\n",
    "        if  self.epsilon >= np.random.rand(1)[0]:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action_values = self.q_network(np.expand_dims(state, axis=(0)))\n",
    "            action = np.argmax(action_values)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        # store training data\n",
    "        self.buffer.add((state, action, reward, next_state, done))\n",
    "    \n",
    "\n",
    "    def train_q_network(self):\n",
    "        # sample\n",
    "        (states, actions, rewards, next_states, dones) = self.buffer.sample(self.batch_size)\n",
    "        future_rewards = self.t_q_network.predict(next_states)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "        \n",
    "        # set last q value to 0\n",
    "        updated_q_values = updated_q_values*(1 - dones)\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # Train the model on the states and updated Q-values\n",
    "          q_values = self.q_network(states)\n",
    "          # only update q-value which is chosen\n",
    "          q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "          # calculate loss between new Q-value and old Q-value\n",
    "          loss = tf.reduce_mean(tf.math.square(q_action - updated_q_values))\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update per update_target_network steps\n",
    "        self.t_q_network.set_weights(self.q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_training(epochs = EPOCHS):\n",
    "    agent = DQN_agent(2,4)\n",
    "    for t in tqdm(range(epochs)):\n",
    "        env = Environment()\n",
    "        state = START\n",
    "        while True:\n",
    "            agent.frame_count += 1\n",
    "            action = agent.ε_greedy_policy(t+1,state)\n",
    "            next_state, reward, done = env.step(state, ACTIONS[action])\n",
    "            \n",
    "            agent.store(state, action, next_state, reward, done)\n",
    "\n",
    "            if agent.frame_count%16 == 0 and len(agent.buffer.buffer) >= agent.batch_size:\n",
    "                agent.train_q_network()\n",
    "            \n",
    "            if agent.frame_count%100 == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            state = next_state\n",
    "            if done == True:\n",
    "                break\n",
    "            \n",
    "        agent.update_target_network()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_optimal_policy(q_network):\n",
    "    # display the optimal policy\n",
    "    optimal_policy = []\n",
    "    for i in range(0, 4):\n",
    "        optimal_policy.append([])\n",
    "        for j in range(0, 12):\n",
    "            if ([i, j] == TARGET).all():\n",
    "                optimal_policy[-1].append('G')\n",
    "                continue\n",
    "            action_values = q_network(np.expand_dims([i,j], axis=(0)))\n",
    "            bestAction = np.argmax(action_values)\n",
    "            if bestAction == 0:\n",
    "                optimal_policy[-1].append('U')\n",
    "            elif bestAction == 1:\n",
    "                optimal_policy[-1].append('D')\n",
    "            elif bestAction == 2:\n",
    "                optimal_policy[-1].append('L')\n",
    "            elif bestAction == 3:\n",
    "                optimal_policy[-1].append('R')\n",
    "    for row in optimal_policy:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [38:15<00:00, 22.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Optimal Policy:\n",
      "['R', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['L', 'U', 'U', 'U', 'L', 'L', 'L', 'L', 'U', 'U', 'U', 'U']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_training()\n",
    "print('Q-Learning Optimal Policy:')\n",
    "print_optimal_policy(agent.q_network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa86f03d865a85f838fe5ab49530f2c4e6314982a13989a911720a66e7fac4c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
